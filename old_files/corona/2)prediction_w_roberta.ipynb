{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from covid import RobertaClassifier, SentimentData\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_name = 'NS_sep-dec2020.csv'\n",
    "df_name ='NS_sep-dec2020.csv'\n",
    "# df_name = 'corona_news_sep-mar2022.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles: 34725\n",
      "CPU times: user 2.28 s, sys: 200 ms, total: 2.48 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df = pd.read_csv('covid_articles_no_txt.csv') \n",
    "# med_df = pd.read_csv('medical_covid_articles.csv')\n",
    "df = pd.read_csv('csv/no_txt_' + df_name) \n",
    "print(f'Number of unique articles: {df.article_id.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'article_id'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[:2] # Drop these since they are just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4651.042840185605"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df.columns[:1],axis=1,inplace=True)\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.dropna(inplace=True)\n",
    "np.mean(df.text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla K80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.conda/envs/torch_rona/lib/python3.8/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ec2-user/SageMaker/.conda/envs/torch_rona/lib/python3.8/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.normalization.LayerNorm' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ec2-user/SageMaker/.conda/envs/torch_rona/lib/python3.8/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ec2-user/SageMaker/.conda/envs/torch_rona/lib/python3.8/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ec2-user/SageMaker/.conda/envs/torch_rona/lib/python3.8/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ec2-user/SageMaker/.conda/envs/torch_rona/lib/python3.8/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Instantiate Finetuned Classifier\n",
    "finetuned_rob = RobertaClassifier().to(device)\n",
    "finetuned_rob.load_state_dict(torch.load('/home/ec2-user/SageMaker/pre_trained_model/covid_checkpoint (1).pth', map_location=device))\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"/home/ec2-user/SageMaker/pre_trained_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2719/203874 [00:01<01:58, 1691.82it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors\n",
      " 55%|█████▌    | 112576/203874 [00:56<00:43, 2087.59it/s]"
     ]
    }
   ],
   "source": [
    "df['len_tokenized'] = df.pairs.progress_apply(lambda sent: len(tokenizer.encode(sent, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only remove these AFTER tokenizing. \n",
    "# bad_articles = df[df.len_tokenized >= 500].article_id\n",
    "# bad_articles.to_csv('csv/bad_articles_'+ df_name)\n",
    "df = df[df['len_tokenized'] < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = df.len_tokenized.max()\n",
    "print(f'Max length of tokenized pair sentences: {MAX_LEN}')\n",
    "print(f'Percentage of sentences with a tokenized length greater than 300 {len(df[df.len_tokenized > 300])/len(df)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_duplicates = np.load(file='duplicates.npy', allow_pickle=True)\n",
    "print(f'Number of duplicate articles: {sum(df.article_id.isin(check_duplicates))}') # Checking for duplicate articles.\n",
    "# df = df[~df.article_id.isin(check_duplicates)] # ONLY RUN THIS FOR TIMES OTHER THAN THE FIRST RUN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a subset of the original dataframe to speed up model runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_df = df[(df.date >= pd.Timestamp(2022,1,1)) & (df.date <= pd.Timestamp(2021,12,31))]\n",
    "# subset_df = df[(df.date < pd.Timestamp(2021,1,1))]\n",
    "\n",
    "# subset_df = df[(df.date < pd.Timestamp(2021,6,1))]\n",
    "# second_subset_df = df[(df.date >= pd.Timestamp(2021,6,1))]\n",
    "\n",
    "subset_df = df # Using entire timeline.\n",
    "subset_df.reset_index(drop=True,inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print(subset_df.article_id.nunique())\n",
    "print(df.article_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predicting with Roberta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rob_predict(model, testing_loader):\n",
    "    model.eval()\n",
    "    prediction_list = []\n",
    "    score_list = []\n",
    "    with torch.no_grad(): \n",
    "        for data in tqdm(testing_loader):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            outputs = F.softmax(model(ids, mask, token_type_ids), dim=-1)\n",
    "            score, prediction = torch.max(outputs.data, dim=1)\n",
    "\n",
    "            prediction_list.extend(prediction.cpu().numpy())\n",
    "            score_list.extend(score.cpu().numpy())\n",
    "        return prediction_list , score_list\n",
    "    \n",
    "def run_model(dataframe):\n",
    "    testing_set = SentimentData(dataframe, tokenizer , max_len = MAX_LEN)\n",
    "    testing_loader = DataLoader(testing_set, batch_size=10, num_workers=0) \n",
    "    prediction_list, score_list = rob_predict(finetuned_rob, testing_loader)\n",
    "    interm = pd.DataFrame({'prediction': prediction_list,'score': score_list})\n",
    "    test_df = pd.concat((dataframe, interm),axis=1)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predicted_df = run_model(dataframe = subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_name = 'B5_sep_dec2020.csv'\n",
    "# df_name = 'B5_jan_mar2022.csv'\n",
    "# df_name = 'Health'+ df_name # 2000 health articles.\n",
    "# df_name ='B5_sep-dec2020.csv'\n",
    "# df_name ='B5_jan-mar2022.csv'\n",
    "\n",
    "predicted_df.to_csv('csv/analysis_' + df_name)\n",
    "predicted_df.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('csv/analysis_' + df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72a60da5162cf5d458cb2db71a4e7796a0c7fc52f842b27edfae3b82254df2c6"
  },
  "kernelspec": {
   "display_name": "torch_rona",
   "language": "python",
   "name": "torch_rona"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
