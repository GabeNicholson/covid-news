{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb289ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "from format_dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79b02b",
   "metadata": {},
   "source": [
    "##### Process Dataset \n",
    "\n",
    "Naming convention: Country_NewsType_article-date-range_(todays date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae00fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Name: us_covid_april_2020-april_2021_(2022-09-19).csv\n"
     ]
    }
   ],
   "source": [
    "country_name = \"us\"\n",
    "news_type = 'covid' # (reg or covid)\n",
    "# news_type = \"reg\"\n",
    "date = \"april_2020-april_2021\"\n",
    "df_name = f\"{country_name}_{news_type}_{date}_({datetime.today().date()}).csv\"\n",
    "print(f\"Dataframe Name: {df_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c05615c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores Available: 4\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'us-no-newsstream-full-timeline'\n",
    "processed_lists = process_dataset(dataset_name, article_skip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab40f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['article_id','date','publisher','title', 'text',\n",
    "         'language', 'page_num', 'source_type',\n",
    "        'city', 'author', 'LexileScore']\n",
    "df = pd.DataFrame(processed_lists, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3068a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504696 Number of articles to analyze. \n",
      " 94958 articles lost.\n"
     ]
    }
   ],
   "source": [
    "before_remove_errors = len(df)\n",
    "df = df[df.text != 'Error in processing document']\n",
    "df = df[df.language == 'English'] \n",
    "df = df[df.source_type != 'Multimedia']\n",
    "df['text'] = df['text'].astype('string')\n",
    "df = df[df.title != 'Cleveland COVID-19 Vaccine Locations']\n",
    "print(f'{len(df)} Number of articles to analyze. \\n {before_remove_errors - len(df)} articles lost.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4620d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df = pd.read_csv(f'cnn us_covid_april_2020-april_2021_(2022-09-18).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee0e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,cnn_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0b5ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb4613",
   "metadata": {},
   "source": [
    "##### filter text/articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea577a5",
   "metadata": {},
   "source": [
    "Covid News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = base_text_formatting(df)\n",
    "df['text_len'] = df.text.str.split().apply(len)\n",
    "df = df[(df.text_len > 325) & (df.text_len < 4000)]\n",
    "\n",
    "sent_tokenizer = nltk.data.load('nltk_tokenizer/punkt/english.pickle')\n",
    "with mp.Pool(3) as pool:\n",
    "    df['sentences'] = pool.map(sent_tokenizer.tokenize, df['text'])\n",
    "print(f'Number of unique articles: {df.article_id.nunique()}')\n",
    "articles_before = df.article_id.nunique()\n",
    "\n",
    "def sentences_keep(sentences):\n",
    "    try:\n",
    "        if len(sentences) >= 19:\n",
    "            # first 15 sentences and last 3 sentences not including the\n",
    "            # last sentence since it is usually an advertisement.\n",
    "            return sentences[:15] + sentences[-4:-1]\n",
    "        elif (len(sentences) <= 19) & (len(sentences) >= 5):\n",
    "            return sentences\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# First we only keep 20 sentences from the article.\n",
    "df.sentences = df.sentences.apply(lambda x: sentences_keep(x)) \n",
    "df.dropna(axis=0, subset=['sentences'], inplace=True)\n",
    "print(f'Articles Lost from being too short: {articles_before - df.article_id.nunique()}')\n",
    "df['filtered_text'] = df['sentences'].str.join(' ')\n",
    "# Then we count the number of keywords in the text.\n",
    "df = experimental_count_keywords(df)\n",
    "df = experimental_covid_article_filtering(df)\n",
    "print(f'Number of unique articles: {df.article_id.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41df3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num=310\n",
    "# print(df.iloc[num])\n",
    "# print(df.iloc[num].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7602c",
   "metadata": {},
   "source": [
    "## For Regular News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = regular_news_formatting(df, num_articles_to_sample=150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f598c8",
   "metadata": {},
   "source": [
    "## For headline Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3001d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = headline_formatting(df)\n",
    "# df.to_csv(f'csv/headline_{df_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('source_type').count()['article_id'] / df.groupby('source_type').count()['article_id'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ed044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_relevant_content(sentences):\n",
    "    copy_sentences = sentences.copy()\n",
    "    check_one = 'Newstex Authoritative Content is not'\n",
    "    check_two = 'The material and information provided in Newstex'\n",
    "    check_three = 'Sign up for our'\n",
    "    check_four = 'Neither newstex nor its re-distributors'\n",
    "    check_five = 'Please wait for the page to reload'\n",
    "    for sentence_num, sentence in enumerate(copy_sentences):\n",
    "        if (bool(re.search(check_one, sentence, re.I)) or \\\n",
    "        bool(re.search(check_two, sentence, re.I)) or \\\n",
    "        bool(re.search(check_three, sentence, re.I)) or \\\n",
    "        bool(re.search(check_four, sentence, re.I)) or \\\n",
    "        bool(re.search(check_five, sentence, re.I))) and (sentence_num >= 9):\n",
    "            \n",
    "            print(f'we got one {sentence_num}')\n",
    "            return sentences[:sentence_num]\n",
    "    return sentences\n",
    "    \n",
    "df['sentences'] = df['sentences'].apply(remove_non_relevant_content) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b177a",
   "metadata": {},
   "source": [
    "This block are past attempts at reformatting to try again once database is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d50b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keep_pairs(lst):\n",
    "    \"\"\" Make sentences into groups of three.\n",
    "    If the article is full length, this will lead to 6 predictions per article.\n",
    "    Grouping makes predictions faster and more accurate. \n",
    "    However, groups larger than 3 will usually go above Roberta's character limit.\n",
    "    \"\"\"\n",
    "    return [' '.join(x) for x in zip(lst[0::3], lst[1::3], lst[2::3])]\n",
    "df['pairs'] = df.sentences.apply(keep_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[(df['keyword_len'] >= 3) & (df['keyword_len'] < 60)]\n",
    "\n",
    "# from transformers import RobertaTokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"/home/ec2-user/SageMaker/pre_trained_tokenizer\")\n",
    "\n",
    "# def check_len(lst_sent):\n",
    "#     token_lst_len = [len(tokenizer.encode(sent, add_special_tokens=True, truncation=True)) for sent in lst_sent]\n",
    "#     sent_cum_len = np.cumsum(token_lst_len)\n",
    "#     idx_lst = np.where(sent_cum_len >= 125)[0]\n",
    "#     if idx_lst.size > 0:\n",
    "#         idx = idx_lst[0]\n",
    "#         return lst_sent[:idx]\n",
    "#     return lst_sent\n",
    "\n",
    "# df['sentences'] = df.sentences.progress_apply(lambda sent: check_len(sent))\n",
    "# df.sentences = df.sentences.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6c197",
   "metadata": {},
   "source": [
    "For Non-Covid News Articles sample from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e351911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6082cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['text', 'language', 'sentences', 'filtered_text'],axis=1,inplace=True)\n",
    "pre_explode = df.drop('pairs', axis=1) # we only use pre_explode as an article_id reference.\n",
    "print(df_name)\n",
    "pre_explode.to_csv('csv/pre_explode_' + df_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('pairs') #This keeps it in the format required for data loader.\n",
    "print(f'Dataframe {df_name} goes from {df.date.min()} to {df.date.max()}.')\n",
    "print(f'Dataframe {df_name} has {df.article_id.nunique()} unique articles.')\n",
    "df.page_num.fillna('None', inplace=True)\n",
    "\n",
    "# df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('csv/no_txt_' + df_name)\n",
    "print(df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cfd8f7",
   "metadata": {},
   "source": [
    "### Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0555665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(100).to_csv('email_test/tdm_samples.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('covid-news')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:07:06) [Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba768a010b84f1c09a4bb22ce6535c84a2bcdedd527dc40cbd91b132a400313e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
