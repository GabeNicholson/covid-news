{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbbd262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from format_dataset import process_dataset, regular_news_formatting, covid_format_text, headline_formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed553e",
   "metadata": {},
   "source": [
    "##### Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "355a945f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ec2-user/SageMaker/data/g-canada-apr2020-apr2021/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7b/prbmv40d5wg4ymrn9dr3c76c0000gn/T/ipykernel_21907/1623887420.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'g-canada-apr2020-apr2021'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocessed_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'can-apr2021-apr2022.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/corona/format_dataset.py\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Define a function to get the text content that is needed from the XML articles available in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetxmlcontent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.//HiddenText'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.//HiddenText'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.//Text'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ec2-user/SageMaker/data/g-canada-apr2020-apr2021/'"
     ]
    }
   ],
   "source": [
    "dataset_name = 'g-canada-apr2020-apr2021'\n",
    "processed_lists = process_dataset(dataset_name)\n",
    "\n",
    "df_name = 'can-apr2021-apr2022.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e676503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659028\n",
      "Number of articles to analyze: 658196\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(processed_lists, columns=['article_id','date','publisher','title', 'text' , 'language', 'page_num', 'source_type'])\n",
    "print(len(df))\n",
    "df = df[df.text != 'Error in processing document']\n",
    "print(f'Number of articles to analyze: {len(df)}')\n",
    "\n",
    "df = df[df.language == 'English'] \n",
    "df = df[df.source_type != 'Multimedia']\n",
    "df['text'] = df['text'].astype('string')\n",
    "df = df[df.title != 'Cleveland COVID-19 Vaccine Locations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189b4af",
   "metadata": {},
   "source": [
    "##### filter text/articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8156566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.text_len < 500].keyword_len.mean()\n",
    "# np.sum(df.text_len>1000) / len(df.text_len)\n",
    "# df.text_len.mean()\n",
    "# df.groupby('source_type').text_len.mean()\n",
    "# df.groupby('source_type').count()['article_id'] / df.groupby('source_type').count()['article_id'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5319a",
   "metadata": {},
   "source": [
    "Covid News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c131b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = covid_format_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e8ae4",
   "metadata": {},
   "source": [
    "For Regular News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = regular_news_formatting(df, num_articles_to_sample=150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f264d49",
   "metadata": {},
   "source": [
    "For headline Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ab88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = headline_formatting(df)\n",
    "# df.to_csv(f'csv/headline_{df_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ca048a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_type\n",
       "Newspapers       0.503319\n",
       "Web Resources    0.496681\n",
       "Name: article_id, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('source_type').count()['article_id'] / df.groupby('source_type').count()['article_id'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c432fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.58 s, sys: 3.37 s, total: 11 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sent_tokenizer = nltk.data.load('nltk_tokenizer/punkt/english.pickle')\n",
    "with mp.Pool(3) as pool:\n",
    "    df['sentences'] = pool.map(sent_tokenizer.tokenize, df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f40fa31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles: 164510\n",
      "Number of unique articles: 164481\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique articles: {df.article_id.nunique()}')\n",
    "def sentences_keep(sentences):\n",
    "    try:\n",
    "        if len(sentences) > 15:\n",
    "            return sentences[:15] \n",
    "        elif (len(sentences) <= 15) & (len(sentences) >= 4):\n",
    "            return sentences\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df.sentences = df.sentences.apply(lambda x: sentences_keep(x)) \n",
    "df.dropna(axis=0, subset=['sentences'], inplace=True)\n",
    "print(f'Number of unique articles: {df.article_id.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_link(sentences):\n",
    "    copy_list = sentences.copy()\n",
    "    check_one = 'Newstex Authoritative Content is not'\n",
    "    check_two = 'The material and information provided in Newstex'\n",
    "    check_three = 'Sign up for our'\n",
    "    check_four = 'Neither newstex nor its re-distributors'\n",
    "    check_five = 'Please wait for the page to reload'\n",
    "    for k, sentence in enumerate(copy_list):\n",
    "        if (bool(re.search(check_one, sentence, re.I)) or \\\n",
    "        bool(re.search(check_two, sentence, re.I)) or \\\n",
    "        bool(re.search(check_three, sentence, re.I)) or \\\n",
    "        bool(re.search(check_four, sentence, re.I)) or \\\n",
    "        bool(re.search(check_five, sentence, re.I))) and (k >= 9):\n",
    "            return sentences[:k]\n",
    "    return sentences\n",
    "    \n",
    "df.sentences = df.sentences.apply(lambda x: email_link(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d079bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164481"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.article_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a6cec",
   "metadata": {},
   "source": [
    "This block are past attempts at reformatting to try again once database is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29458256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keep_pairs(lst):\n",
    "    \"\"\" Makes predictions on pairs of sentences since they are usually small enough and fixes issues of tiny sentences.\n",
    "    \"\"\"\n",
    "    return [' '.join(x) for x in zip(lst[0::3], lst[1::3], lst[2::3])]\n",
    "df['pairs'] = df.sentences.apply(keep_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b719e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[(df['keyword_len'] >= 3) & (df['keyword_len'] < 60)]\n",
    "\n",
    "# from transformers import RobertaTokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"/home/ec2-user/SageMaker/pre_trained_tokenizer\")\n",
    "\n",
    "# def check_len(lst_sent):\n",
    "#     token_lst_len = [len(tokenizer.encode(sent, add_special_tokens=True, truncation=True)) for sent in lst_sent]\n",
    "#     sent_cum_len = np.cumsum(token_lst_len)\n",
    "#     idx_lst = np.where(sent_cum_len >= 125)[0]\n",
    "#     if idx_lst.size > 0:\n",
    "#         idx = idx_lst[0]\n",
    "#         return lst_sent[:idx]\n",
    "#     return lst_sent\n",
    "\n",
    "# df['sentences'] = df.sentences.progress_apply(lambda sent: check_len(sent))\n",
    "# df.sentences = df.sentences.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad6646",
   "metadata": {},
   "source": [
    "For Non-Covid News Articles sample from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b853984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can-apr2021-apr2022.csv\n"
     ]
    }
   ],
   "source": [
    "df = df[['article_id', 'date', 'publisher', 'title', 'page_num', 'pairs']]\n",
    "pre_explode = df.drop('pairs', axis=1)\n",
    "\n",
    "print(df_name)\n",
    "pre_explode.to_csv('csv/pre_explode_' + df_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea53129",
   "metadata": {},
   "source": [
    "Make sure to concatenate all the other files other than the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99cd15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(file='duplicates.npy', arr=df.article_id.unique())\n",
    "# duplicates = np.load(file='duplicates.npy', allow_pickle=True)\n",
    "# updated_duplicate = np.concatenate((duplicates, df.article_id))\n",
    "# np.save(file='duplicates.npy', arr=updated_duplicate)\n",
    "# check = np.load(file='duplicates.npy', allow_pickle=True)\n",
    "# df.article_id.isin(check).sum()\n",
    "# df = df[~df.article_id.isin(check)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5623f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe can-apr2021-apr2022.csv goes from 2020-04-01 to 2021-03-31.\n",
      "Dataframe can-apr2021-apr2022.csv has 164481 unique articles.\n"
     ]
    }
   ],
   "source": [
    "df = df.explode('pairs') #This keeps it in the format required for data loader.\n",
    "print(f'Dataframe {df_name} goes from {df.date.min()} to {df.date.max()}.')\n",
    "print(f'Dataframe {df_name} has {df.article_id.nunique()} unique articles.')\n",
    "df.page_num.fillna('None', inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2364f789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can-apr2021-apr2022.csv'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('csv/no_txt_' + df_name)\n",
    "df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ec75b",
   "metadata": {},
   "source": [
    "### Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790aac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(100).to_csv('email_test/tdm_samples.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9538652c875555cca96669dfca2072ebce91459703d44f7fca3d59b8ec9280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
